import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, average_precision_score

# ---------------------------------------------
# Helper Functions
# ---------------------------------------------
def binarize_labels(y_true, y_pred_labels):
    labels = sorted(list(set(y_true) | set(y_pred_labels)))
    label_map = {label: i for i, label in enumerate(labels)}
    return [label_map[y] for y in y_true], [label_map[y] for y in y_pred_labels]

# ---------------------------------------------
# 1. Accuracy, AUC-ROC, AUC-PR Across Models
# ---------------------------------------------
metrics = []

for name, df in datasets.items():
    df = df[df["Intent"] != "FallbackIntent"].copy()

    for model, (pred_col, conf_col) in {
        "Lex": ("MappedIntent", "LexConfidence"),
        "Baseline": ("BaselinePrediction", "BaselineConfidence"),
        "Bert": ("BertFinalIntent", "BertConfidence")
    }.items():

        if pred_col not in df.columns or conf_col not in df.columns:
            continue

        valid_rows = df[["Intent", pred_col, conf_col]].dropna()
        y_true_raw = valid_rows["Intent"]
        y_pred_raw = valid_rows[pred_col]
        y_conf = valid_rows[conf_col].astype(float)

        y_true_enc, y_pred_enc = binarize_labels(y_true_raw, y_pred_raw)
        accuracy = (y_pred_raw == y_true_raw).mean()

        try:
            auc_roc = roc_auc_score(y_true_enc, y_conf, multi_class="ovr")
        except:
            auc_roc = None
        try:
            auc_pr = average_precision_score(pd.get_dummies(y_true_enc), pd.get_dummies(y_pred_enc), average="macro")
        except:
            auc_pr = None

        metrics.append({
            "Dataset": name,
            "Model": model,
            "Accuracy": accuracy,
            "AUC-ROC": auc_roc,
            "AUC-PR": auc_pr
        })

metrics_df = pd.DataFrame(metrics)

melted = metrics_df.melt(
    id_vars=["Dataset", "Model"],
    value_vars=["Accuracy", "AUC-ROC", "AUC-PR"],
    var_name="Metric",
    value_name="Value"
)

g = sns.catplot(
    data=melted,
    x="Dataset",
    y="Value",
    hue="Model",
    col="Metric",
    kind="bar",
    height=4,
    aspect=1.3,
    ci=None
)
g.set_titles("{col_name}")
g.set_axis_labels("Dataset", "Score")
g.set(ylim=(0, 1))
g.fig.subplots_adjust(top=0.85)
g.fig.suptitle("Model Comparison: Accuracy, AUC-ROC, AUC-PR by Dataset", fontsize=14)

# ---------------------------------------------
# 2. Relative Lift of Bert over Lex
# ---------------------------------------------
lift_df = metrics_df.pivot(index="Dataset", columns="Model", values="Accuracy").reset_index()
lift_df["Relative Lift (Bert vs Lex)"] = (lift_df["Bert"] - lift_df["Lex"]) / lift_df["Lex"]

plt.figure(figsize=(8, 5))
sns.barplot(data=lift_df, x="Dataset", y="Relative Lift (Bert vs Lex)", color="green")
plt.axhline(0, linestyle="--", color="gray")
plt.title("Relative Accuracy Lift: Bert over Lex")
plt.ylabel("Relative Lift (%)")
plt.xlabel("Dataset")
plt.grid(True)
plt.tight_layout()
plt.show()

# ---------------------------------------------
# 3. Fallback Rate Comparison
# ---------------------------------------------
fallback_rates = []
CONF_THRESHOLD = 0.55

for name, df in datasets.items():
    for model, conf_col in {
        "Lex": "LexConfidence",
        "Baseline": "BaselineConfidence",
        "Bert": "BertConfidence"
    }.items():
        if conf_col not in df.columns:
            continue
        fallback = (df[conf_col].astype(float) < CONF_THRESHOLD).mean()
        fallback_rates.append({
            "Dataset": name,
            "Model": model,
            "Fallback Rate": fallback
        })

fb_df = pd.DataFrame(fallback_rates)
plt.figure(figsize=(10, 5))
sns.barplot(data=fb_df, x="Dataset", y="Fallback Rate", hue="Model")
plt.title("Fallback Rate Comparison (Confidence < 0.55)")
plt.ylabel("Fallback Rate")
plt.xlabel("Dataset")
plt.ylim(0, 1)
plt.grid(True)
plt.tight_layout()
plt.show()

# ---------------------------------------------
# 4. Top-1 vs Top-2 Accuracy (Bert Only)
# ---------------------------------------------
topk_results = []

for name, df in datasets.items():
    if "BertTop2Intent" not in df.columns:
        continue

    total = len(df)
    top1 = (df["BertFinalIntent"] == df["Intent"]).sum()
    top2 = ((df["BertFinalIntent"] == df["Intent"]) | (df["BertTop2Intent"] == df["Intent"])).sum()

    topk_results.append({"Dataset": name, "TopK": "Top-1", "Accuracy": top1 / total})
    topk_results.append({"Dataset": name, "TopK": "Top-2", "Accuracy": top2 / total})

topk_df = pd.DataFrame(topk_results)

plt.figure(figsize=(8, 5))
sns.barplot(data=topk_df, x="Dataset", y="Accuracy", hue="TopK")
plt.title("Bert Top-1 vs Top-2 Accuracy by Dataset")
plt.ylabel("Accuracy")
plt.xlabel("Dataset")
plt.ylim(0, 1)
plt.grid(True)
plt.tight_layout()
plt.show()
