import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Generate or load your dataset
np.random.seed(0)
X = pd.DataFrame(np.random.rand(100, 46), columns=[f"feature_{i}" for i in range(46)])

# Scale the dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Hyperparameters
n_clusters = 3  # Number of clusters
feature_reduction_ratio = 0.9  # Reduce encoder dimensions by 10% each step
max_no_improvement_steps = 3  # Early stopping threshold

# Track metrics and selected features
stepwise_ss = []
stepwise_wcss = []
selected_features_stepwise = []

# Initial set of features
remaining_features = list(X.columns)
no_improvement_steps = 0
best_ss = -1
current_encoder_dim = len(remaining_features)  # Start with full dimension

while current_encoder_dim > 1:
    # Define the autoencoder with dynamic encoding dimension
    input_layer = Input(shape=(len(remaining_features),))
    encoder = Dense(current_encoder_dim, activation="relu")(input_layer)
    decoder = Dense(len(remaining_features), activation="linear")(encoder)

    # Compile the autoencoder model
    autoencoder = Model(inputs=input_layer, outputs=decoder)
    autoencoder.compile(optimizer=Adam(), loss='mse')

    # Train the autoencoder on the current subset of features
    X_subset = X[remaining_features]
    X_subset_scaled = scaler.fit_transform(X_subset)
    autoencoder.fit(X_subset_scaled, X_subset_scaled, epochs=50, batch_size=10, verbose=0)

    # Extract encoder weights and determine the top features to retain
    encoder_weights = autoencoder.layers[1].get_weights()[0]
    feature_importance = np.sum(np.abs(encoder_weights), axis=1)
    n_features_to_keep = int(len(remaining_features) * feature_reduction_ratio)
    top_feature_indices = np.argsort(feature_importance)[-n_features_to_keep:]
    selected_features = [remaining_features[i] for i in top_feature_indices]
    selected_features_stepwise.append(selected_features)

    # Update the remaining features list
    remaining_features = selected_features
    current_encoder_dim = int(len(remaining_features) * feature_reduction_ratio)

    # Clustering on the selected features
    X_selected_scaled = scaler.fit_transform(X[selected_features])
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    labels = kmeans.fit_predict(X_selected_scaled)

    # Calculate Silhouette Score and WCSS
    if len(np.unique(labels)) > 1:
        ss = silhouette_score(X_selected_scaled, labels)
        wcss = kmeans.inertia_

        # Track metrics
        stepwise_ss.append(ss)
        stepwise_wcss.append(wcss)

        # Early stopping logic
        if ss > best_ss:
            best_ss = ss
            no_improvement_steps = 0
        else:
            no_improvement_steps += 1
            if no_improvement_steps >= max_no_improvement_steps:
                break
    else:
        # Break if clustering fails
        print("Only one cluster formed; stopping.")
        break

# Plot Silhouette Score and WCSS over feature reduction steps
plt.figure(figsize=(12, 6))
plt.plot(range(len(stepwise_ss)), stepwise_ss, marker='o', label="Silhouette Score")
plt.plot(range(len(stepwise_wcss)), stepwise_wcss, marker='s', label="WCSS (Inertia)")
plt.xlabel("Feature Reduction Steps")
plt.ylabel("Score")
plt.legend()
plt.title("Clustering Quality over Stepwise Feature Reduction")
plt.grid(True)
plt.show()

# Print the final set of selected features for each step
for i, features in enumerate(selected_features_stepwise):
    print(f"Step {i+1}: Selected Features ({len(features)}) - {features}")
