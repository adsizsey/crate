import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Generate or load your dataset
np.random.seed(0)
X = pd.DataFrame(np.random.rand(100, 46), columns=[f"feature_{i}" for i in range(46)])

# Scale the dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Hyperparameters
n_clusters = 3  # Number of clusters
feature_reduction_ratio = 0.9  # Reduce encoder dimensions by 10% each step
max_no_improvement_steps = 3  # Early stopping threshold

# Track metrics and selected features
stepwise_ss = []
stepwise_wcss = []
selected_features_stepwise = []

# Initialize encoder dimension based on total number of features
current_encoder_dim = X.shape[1]  # Start with all features
no_improvement_steps = 0
best_ss = -1

while current_encoder_dim >= 1:
    # Define the autoencoder with dynamic encoding dimension
    input_layer = Input(shape=(X.shape[1],))  # Fixed input layer size
    encoder = Dense(current_encoder_dim, activation="relu")(input_layer)
    decoder = Dense(X.shape[1], activation="linear")(encoder)
    
    # Compile the autoencoder model
    autoencoder = Model(inputs=input_layer, outputs=decoder)
    autoencoder.compile(optimizer=Adam(), loss='mse')
    
    # Train the autoencoder on the full set of features
    autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=10, verbose=0)
    
    # Extract encoder weights and determine the top features to retain
    encoder_weights = autoencoder.layers[1].get_weights()[0]
    feature_importance = np.sum(np.abs(encoder_weights), axis=1)
    
    # Sort and keep the top features based on encoding size
    top_feature_indices = np.argsort(feature_importance)[-current_encoder_dim:]
    selected_features = [X.columns[i] for i in top_feature_indices]
    selected_features_stepwise.append(selected_features)
    
    # Clustering on the selected features
    X_selected_scaled = scaler.fit_transform(X[selected_features])
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    labels = kmeans.fit_predict(X_selected_scaled)
    
    # Calculate Silhouette Score and WCSS
    if len(np.unique(labels)) > 1:
        ss = silhouette_score(X_selected_scaled, labels)
        wcss = kmeans.inertia_
        
        # Track metrics
        stepwise_ss.append(ss)
        stepwise_wcss.append(wcss)
        
        # Early stopping logic
        if ss > best_ss:
            best_ss = ss
            no_improvement_steps = 0
        else:
            no_improvement_steps += 1
            if no_improvement_steps >= max_no_improvement_steps:
                break
    else:
        print("Only one cluster formed; stopping.")
        break
    
    # Update encoder dimension to reduce by 10% for the next step
    current_encoder_dim = int(current_encoder_dim * feature_reduction_ratio)
    if current_encoder_dim < 1:
        break

# Plot Silhouette Score and WCSS with the X-axis as the number of features kept
features_kept = [len(features) for features in selected_features_stepwise]

plt.figure(figsize=(12, 6))
plt.plot(features_kept, stepwise_ss, marker='o', label="Silhouette Score")
plt.plot(features_kept, stepwise_wcss, marker='s', label="WCSS (Inertia)")
plt.xlabel("Number of Features Kept")
plt.ylabel("Score")
plt.legend()
plt.title("Clustering Quality over Number of Features Kept")
plt.grid(True)
plt.show()
