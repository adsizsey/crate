import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

# Load datasets
datasets = {
    "Original Dataset Test": None,  # Placeholder for Original Dataset Test data to exclude from training
    "Informal": pd.read_csv("informal_results.csv"),
    "Broken": pd.read_csv("broken_results.csv"),
    "Contextual": pd.read_csv("contextual_results.csv"),
    "Passive": pd.read_csv("passive_results.csv"),
    "Synonym": pd.read_csv("synonym_results.csv"),
}

# Text cleaning function
def clean_text(text):
    import re
    text = re.sub(r"[^a-zA-Z\s]", "", text)  # Keep only letters and spaces
    text = re.sub(r"\s+", " ", text)         # Remove extra spaces
    return text.strip().lower()

# Apply text cleaning
for name, df in datasets.items():
    if df is not None:
        df['Utterance'] = df['Utterance'].apply(clean_text)

# Split Original Dataset into train/test sets
original_data = pd.read_csv("lex_results.csv")
original_data['Utterance'] = original_data['Utterance'].apply(clean_text)

train_data, original_test_data = train_test_split(
    original_data, test_size=0.3, stratify=original_data['ExpectedIntent'], random_state=42
)
datasets["Original Dataset Test"] = original_test_data

# Helper function for evaluation
def evaluate_model(y_true, y_pred, model_name, dataset_name):
    return {
        "Model": model_name,
        "Dataset": dataset_name,
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, average='weighted', zero_division=0),
        "Recall": recall_score(y_true, y_pred, average='weighted', zero_division=0),
    }

# Initialize results list
results = []

# ------------------------------------
# Baseline Model (Logistic Regression)
# ------------------------------------
vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_features=5000)
X_train = vectorizer.fit_transform(train_data['Utterance'])
y_train = train_data['ExpectedIntent']

baseline_model = LogisticRegression(max_iter=1000, random_state=42)
baseline_model.fit(X_train, y_train)

# Evaluate on unseen datasets only
for name, df in datasets.items():
    if df is not None:  # Skip train data
        X_test = vectorizer.transform(df['Utterance'])
        y_test = df['ExpectedIntent']
        y_pred = baseline_model.predict(X_test)
        results.append(evaluate_model(y_test, y_pred, "Baseline", name))

# -----------------
# RNN Model (TensorFlow)
# -----------------
# Tokenization and padding
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data['Utterance'])

vocab_size = len(tokenizer.word_index) + 1
max_len = 50

def prepare_data(data, tokenizer, label_map):
    sequences = tokenizer.texts_to_sequences(data['Utterance'])
    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')
    labels = data['ExpectedIntent'].map(label_map).values
    return padded_sequences, labels

# Prepare train/test data
label_map = {label: idx for idx, label in enumerate(train_data['ExpectedIntent'].unique())}
reverse_label_map = {idx: label for label, idx in label_map.items()}

X_train_rnn, y_train_rnn = prepare_data(train_data, tokenizer, label_map)

# Load pre-trained GloVe embeddings
embedding_dim = 100
glove_path = "glove.6B.100d.txt"  # Update with your path to GloVe embeddings
embedding_index = {}

with open(glove_path, "r", encoding="utf-8") as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype="float32")
        embedding_index[word] = coefs

# Create embedding matrix
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    embedding_vector = embedding_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Build RNN model with pre-trained embeddings
rnn_model = Sequential([
    Input(shape=(max_len,)),
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),
    LSTM(128, return_sequences=False),
    Dropout(0.5),
    Dense(len(label_map), activation='softmax')
])

rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
rnn_model.summary()

# Train the RNN model
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
rnn_model.fit(
    X_train_rnn, y_train_rnn,
    epochs=10,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping]
)

# Evaluate RNN on unseen datasets only
for name, df in datasets.items():
    if df is not None:  # Skip train data
        X_test_rnn, y_test_rnn = prepare_data(df, tokenizer, label_map)
        y_pred_rnn_probs = rnn_model.predict(X_test_rnn)
        y_pred_rnn = np.argmax(y_pred_rnn_probs, axis=1)
        y_pred_rnn_labels = [reverse_label_map[idx] for idx in y_pred_rnn]
        y_test_rnn_labels = [reverse_label_map[idx] for idx in y_test_rnn]
        results.append(evaluate_model(y_test_rnn_labels, y_pred_rnn_labels, "RNN", name))

# -----------------
# Vendor Model
# -----------------
for name, df in datasets.items():
    if df is not None:  # Skip train data
        y_true = df['ExpectedIntent']
        y_pred = df['MappedIntent']  # Vendor predictions are in MappedIntent column
        results.append(evaluate_model(y_true, y_pred, "Vendor", name))

# -----------------
# Create Summary Table
# -----------------
results_df = pd.DataFrame(results)
print(results_df)
