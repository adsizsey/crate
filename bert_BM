import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Input, Masking
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load datasets
datasets = {
    "Lex_Test": None,  # Placeholder for Lex test data to exclude from training
    "Informal": pd.read_csv("informal_results.csv"),
    "Broken": pd.read_csv("broken_results.csv"),
    "Contextual": pd.read_csv("contextual_results.csv"),
    "Passive": pd.read_csv("passive_results.csv"),
    "Synonym": pd.read_csv("synonym_results.csv"),
}

# Text cleaning function
def clean_text(text):
    import re
    text = re.sub(r"[^a-zA-Z\s]", "", text)  # Keep only letters and spaces
    text = re.sub(r"\s+", " ", text)         # Remove extra spaces
    return text.strip().lower()

# Apply text cleaning
for name, df in datasets.items():
    if df is not None:
        df['Utterance'] = df['Utterance'].apply(clean_text)

# Split Lex dataset into train/test sets
lex_data = pd.read_csv("lex_results.csv")
lex_data['Utterance'] = lex_data['Utterance'].apply(clean_text)

train_data, lex_test_data = train_test_split(
    lex_data, test_size=0.3, stratify=lex_data['ExpectedIntent'], random_state=42
)
datasets["Lex_Test"] = lex_test_data

# Helper function for evaluation
def evaluate_model(y_true, y_pred, model_name, dataset_name):
    return {
        "Model": model_name,
        "Dataset": dataset_name,
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, average='weighted', zero_division=0),
        "Recall": recall_score(y_true, y_pred, average='weighted', zero_division=0),
    }

# Initialize results list
results = []

# ------------------------------------
# Baseline Model (Logistic Regression)
# ------------------------------------
vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_features=5000)
X_train = vectorizer.fit_transform(train_data['Utterance'])
y_train = train_data['ExpectedIntent']

baseline_model = LogisticRegression(max_iter=1000, random_state=42)
baseline_model.fit(X_train, y_train)

# Evaluate on unseen datasets only
for name, df in datasets.items():
    if df is not None:  # Skip train data
        X_test = vectorizer.transform(df['Utterance'])
        y_test = df['ExpectedIntent']
        y_pred = baseline_model.predict(X_test)
        results.append(evaluate_model(y_test, y_pred, "Baseline", name))

# -----------------
# RNN Model (TensorFlow)
# -----------------
# Tokenize and pad sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_data['Utterance'])

vocab_size = len(tokenizer.word_index) + 1
max_len = 50

def prepare_data(data, tokenizer, label_map):
    sequences = tokenizer.texts_to_sequences(data['Utterance'])
    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')
    labels = data['ExpectedIntent'].map(label_map).values
    return padded_sequences, labels

# Prepare train/test data
label_map = {label: idx for idx, label in enumerate(train_data['ExpectedIntent'].unique())}
reverse_label_map = {idx: label for label, idx in label_map.items()}

X_train_rnn, y_train_rnn = prepare_data(train_data, tokenizer, label_map)

# Build the RNN model
rnn_model = Sequential([
    Input(shape=(max_len,)),
    Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True),
    SimpleRNN(128, return_sequences=False),
    Dense(len(label_map), activation='softmax')
])

rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
rnn_model.summary()

# Train the RNN model
rnn_model.fit(X_train_rnn, y_train_rnn, epochs=5, batch_size=32, validation_split=0.2)

# Evaluate RNN on unseen datasets only
for name, df in datasets.items():
    if df is not None:  # Skip train data
        X_test_rnn, y_test_rnn = prepare_data(df, tokenizer, label_map)
        y_pred_rnn_probs = rnn_model.predict(X_test_rnn)
        y_pred_rnn = np.argmax(y_pred_rnn_probs, axis=1)
        y_pred_rnn_labels = [reverse_label_map[idx] for idx in y_pred_rnn]
        y_test_rnn_labels = [reverse_label_map[idx] for idx in y_test_rnn]
        results.append(evaluate_model(y_test_rnn_labels, y_pred_rnn_labels, "RNN", name))

# -----------------
# Vendor Model
# -----------------
for name, df in datasets.items():
    if df is not None:  # Skip train data
        y_true = df['ExpectedIntent']
        y_pred = df['MappedIntent']  # Vendor predictions are in MappedIntent column
        results.append(evaluate_model(y_true, y_pred, "Vendor", name))

# -----------------
# Create Summary Table
# -----------------
results_df = pd.DataFrame(results)
print(results_df)
